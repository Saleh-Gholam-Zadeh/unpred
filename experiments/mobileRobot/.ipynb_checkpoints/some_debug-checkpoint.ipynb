{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af2a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fda1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('.')\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "from hydra.utils import get_original_cwd\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1230a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13b93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class metaMobileData(Dataset):\n",
    "    def __init__(self, data_cfg=None):\n",
    "        if data_cfg is None:\n",
    "            raise Exception('Please specify a valid Confg for data')\n",
    "        else:\n",
    "            self.c = data_cfg\n",
    "        if self.c.terrain=='sin2':\n",
    "            self._dataFolder = get_original_cwd() + '/data/MobileRobot/sin2/'\n",
    "            if self.c.frequency == '500':\n",
    "                self._trajectoryPath = self._dataFolder + 'ts_002_50x2000_w_grad.npz'\n",
    "            if self.c.frequency == '240':\n",
    "                self._trajectoryPath = self._dataFolder + 'ts_def_50x1000_w_grad.npz'\n",
    "        else:\n",
    "            self._dataFolder = get_original_cwd() + '/data/MobileRobot/sinMix2/'\n",
    "            if self.c.frequency == '500':\n",
    "                self._trajectoryPath = self._dataFolder + 'ts_002_50x2000_w_grad.npz'\n",
    "            if self.c.frequency == '240':\n",
    "                self._trajectoryPath = self._dataFolder + 'ts_def_50x1000_w_grad.npz'\n",
    "\n",
    "        self._save_windows = self.c.save\n",
    "        self._load_windows = self.c.load\n",
    "        self.dim = self.c.dim\n",
    "        self.trajPerTask = self.c.trajPerTask\n",
    "\n",
    "        self.tar_type = self.c.tar_type\n",
    "\n",
    "        self._split = OmegaConf.to_container(self.c.split)\n",
    "        self._shuffle_split = self.c.shuffle_split\n",
    "\n",
    "        self.meta_batch_size = self.c.meta_batch_size\n",
    "        self.batch_size = self.c.batch_size  # window length/2\n",
    "        self.normalization = None\n",
    "        self.filename = self.c.file_name\n",
    "        self.standardize = self.c.standardize\n",
    "        self.train_windows, self.test_windows = self._load_data()\n",
    "        # data_windows = {'obs': obs_batch, 'act': act_batch, 'target': target_batch, 'obs_valid':obs_valid_batch}\n",
    "\n",
    "    def normalize(self, data, mean, std):\n",
    "        dim = data.shape[-1]\n",
    "        return (data - mean[:self.dim]) / (std[:self.dim] + 1e-10)\n",
    "\n",
    "    def denormalize(self, data, mean, std):\n",
    "        dim = data.shape[-1]\n",
    "        return data * (std[:self.dim] + 1e-10) + mean[:self.dim]\n",
    "\n",
    "    def get_statistics(self, data, dim, difference=False):\n",
    "        if difference:\n",
    "            data = (data[:, 1:, :self.dim] - data[:, :-1, :self.dim])\n",
    "        reshape = lambda x: np.reshape(x, (x.shape[0] * x.shape[1], -1))\n",
    "        data = reshape(data);\n",
    "        mean = np.mean(data, axis=0)\n",
    "        std = np.std(data, axis=0)\n",
    "        return mean, std\n",
    "\n",
    "    def _load_data(self):\n",
    "        # load the pickle file of trajectories\n",
    "        if self._load_windows is not None:\n",
    "            train_data_window = pickle.load(open(self._dataFolder + self.filename + '_train.pickle', 'rb'))\n",
    "            test_data_window = pickle.load(open(self._dataFolder + self.filename + '_test.pickle', 'rb'))\n",
    "            self.normalization = train_data_window['normalization']\n",
    "            print('>>>>>>>>>>>>Loaded Saved Windows with shape<<<<<<<<<<<<<<<', train_data_window['obs'].shape)\n",
    "\n",
    "        else:\n",
    "            data_np = np.load(self._trajectoryPath) #load the .npz data fr\n",
    "            print('>>>>>>>>>>>>Loaded Data Trajectories with shape<<<<<<<<<<<<<<<', data_np['pos'].shape)\n",
    "\n",
    "            # collect obs, act, next states\n",
    "            data = {'observations':[], 'actions':[], 'next_observations':[]}\n",
    "            # dim = np.r_[0:15, 30:41]\n",
    "            data['observations'] = np.concatenate((data_np['pos'][:,:-1,:3],np.sin(data_np['orn_euler'])[:,:-1,:],np.cos(data_np['orn_euler'])[:,:-1,:]),axis=-1)\n",
    "            data['actions'] = data_np['jointAppliedTorques'][:,:-1, :]\n",
    "            data['grad'] = data_np['pos'][:, :-1, 3]\n",
    "            data['next_observations'] = np.concatenate((data_np['pos'][:,1:,:3],np.sin(data_np['orn_euler'])[:,1:,:],np.cos(data_np['orn_euler'])[:,1:,:]),axis=-1)\n",
    "            obs = data['observations']\n",
    "            print('>>>>>>>>>>>>Processed Data Trajectories with shape<<<<<<<<<<<<<<<', obs.shape,data_np['jointAppliedTorques'].shape)\n",
    "            act = data['actions']\n",
    "            next_obs = data['next_observations']\n",
    "            grad = data[\"grad\"]\n",
    "\n",
    "            # train test split\n",
    "            train_obs, train_act, train_next_obs, train_grad, test_obs, test_act, test_next_obs, test_grad = self.train_test_split(obs, act,\n",
    "                                                                                                            next_obs,grad)\n",
    "            train_delta = train_next_obs - train_obs\n",
    "            test_delta = test_next_obs - test_obs\n",
    "\n",
    "            # get different statistics for state, actions, delta_state, delta_action and residuals which will be used for standardization\n",
    "            mean_state_diff, std_state_diff = self.get_statistics(train_delta, self.dim, difference=True)\n",
    "            mean_obs, std_obs = self.get_statistics(train_obs, self.dim)\n",
    "            mean_act, std_act = self.get_statistics(train_act, 2 * self.dim)\n",
    "            self.normalization = dict()\n",
    "            self.normalization['observations'] = [mean_obs, std_obs]\n",
    "            self.normalization['actions'] = [mean_act, std_act]\n",
    "            self.normalization['diff'] = [mean_state_diff, std_state_diff]\n",
    "\n",
    "            # compute delta\n",
    "            if self.tar_type == 'delta':\n",
    "                print(\">>>>>>>>>>>>>>>>>>>>>>>>>>> Training On Differences <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "                self.normalization['targets'] = [mean_state_diff, std_state_diff]\n",
    "            else:\n",
    "                print(\n",
    "                    \">>>>>>>>>>>>>>>>>>>>>>>>>>> Training On Next States(not differences) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "                self.normalization['targets'] = [mean_obs, std_obs]\n",
    "\n",
    "            # Standardize\n",
    "            if self.standardize:\n",
    "                print(\">>>>>>>>>>>>>>>>>>>>>>>>>Standardizing The Data<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "\n",
    "                self.train_obs = self.normalize(train_obs, self.normalization[\"observations\"][0],\n",
    "                                                self.normalization[\"observations\"][1])\n",
    "                self.train_acts = self.normalize(train_act, self.normalization[\"actions\"][0],\n",
    "                                                 self.normalization[\"actions\"][1])\n",
    "\n",
    "                self.test_obs = self.normalize(test_obs, self.normalization[\"observations\"][0],\n",
    "                                               self.normalization[\"observations\"][1])\n",
    "                self.test_acts = self.normalize(test_act, self.normalization[\"actions\"][0],\n",
    "                                                self.normalization[\"actions\"][1])\n",
    "\n",
    "                if self.tar_type == 'delta':\n",
    "                    self.train_targets = self.normalize(train_delta, self.normalization[\"diff\"][0],\n",
    "                                                        self.normalization[\"diff\"][1])\n",
    "                    self.test_targets = self.normalize(test_delta, self.normalization[\"diff\"][0],\n",
    "                                                       self.normalization[\"diff\"][1])\n",
    "                else:\n",
    "                    self.train_targets = self.normalize(train_next_obs, self.normalization[\"observations\"][0],\n",
    "                                                        self.normalization[\"observations\"][1])\n",
    "                    self.test_targets = self.normalize(test_next_obs, self.normalization[\"observations\"][0],\n",
    "                                                       self.normalization[\"observations\"][1])\n",
    "\n",
    "\n",
    "            else:\n",
    "                self.train_obs = train_obs\n",
    "                self.train_acts = train_act\n",
    "                self.test_obs = test_obs\n",
    "                self.test_acts = test_act\n",
    "                if self.tar_type == 'delta':\n",
    "                    self.train_targets = train_delta\n",
    "                    self.test_targets = test_delta\n",
    "                else:\n",
    "                    self.train_targets = train_next_obs\n",
    "                    self.test_targets = test_next_obs\n",
    "            self.train_task_idx = train_grad\n",
    "            self.test_task_idx = test_grad\n",
    "\n",
    "            # Get random windows\n",
    "            train_data_window = self._get_batch(train=True)\n",
    "            test_data_window = self._get_batch(train=False)\n",
    "            if self._save_windows is not None:\n",
    "                pickle.dump(train_data_window, open(self._dataFolder + self.filename + '_train.pickle', 'wb'))\n",
    "                pickle.dump(test_data_window, open(self._dataFolder + self.filename + '_test.pickle', 'wb'))\n",
    "\n",
    "        if isinstance(self._shuffle_split, float):\n",
    "            dataset_size = train_data_window['obs'].shape[0]\n",
    "            indices = np.arange(dataset_size)\n",
    "            print(indices, dataset_size)\n",
    "            np.random.shuffle(indices)\n",
    "            print(indices)\n",
    "            split_idx = int(dataset_size * self._shuffle_split)\n",
    "            idx_train = indices[:split_idx]\n",
    "            idx_test = indices[split_idx:]\n",
    "            train_set = {'obs': [], 'act': [], 'target': [], 'task_index': [], 'normalization': []}\n",
    "            test_set = {'obs': [], 'act': [], 'target': [], 'task_index': [], 'normalization': []}\n",
    "            train_set['obs'] = train_data_window['obs'][idx_train,:,:3];\n",
    "            test_set['obs'] = train_data_window['obs'][idx_test,:,:3];\n",
    "            train_set['act'] = train_data_window['act'][idx_train];\n",
    "            test_set['act'] = train_data_window['act'][idx_test];\n",
    "            train_set['target'] = train_data_window['target'][idx_train,:,:3];\n",
    "            test_set['target'] = train_data_window['target'][idx_test,:,:3];\n",
    "            train_set['task_index'] = train_data_window['task_index'][idx_train,:,3];\n",
    "            test_set['task_index'] = train_data_window['task_index'][idx_test,:,3];\n",
    "            train_set['normalization'] = self.normalization;\n",
    "            test_set['normalization'] = self.normalization\n",
    "            print('Train Test Split Ratio', self._shuffle_split)\n",
    "            return train_set, test_set\n",
    "\n",
    "        return train_data_window, test_data_window\n",
    "\n",
    "\n",
    "    def _get_batch(self, train, percentage_imputation=0.0):\n",
    "        # Takes multiple paths and splits them into windows based on random locations within a trajectory\n",
    "        if train:\n",
    "            num_paths, len_path = self.train_obs.shape[:2]\n",
    "            idx_path = np.random.randint(0, num_paths,\n",
    "                                         size=self.meta_batch_size)  # task index, which gets mixed along the\n",
    "            # process\n",
    "            idx_batch = np.random.randint(self.batch_size, len_path - self.batch_size, size=self.meta_batch_size)\n",
    "\n",
    "            obs_batch = np.array([self.train_obs[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "            act_batch = np.array([self.train_acts[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "            target_batch = np.array([self.train_targets[ip,\n",
    "                                     ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                     for ip, ib in zip(idx_path, idx_batch)])\n",
    "            t_idx_batch = np.array([self.train_task_idx[ip,\n",
    "                                    ib - self.batch_size:ib + self.batch_size]\n",
    "                                    for ip, ib in zip(idx_path, idx_batch)])\n",
    "\n",
    "        else:\n",
    "            num_paths, len_path = self.test_obs.shape[:2]\n",
    "            idx_path = np.random.randint(0, num_paths, size=self.meta_batch_size)\n",
    "            idx_batch = np.random.randint(self.batch_size, len_path - self.batch_size, size=self.meta_batch_size)\n",
    "\n",
    "            obs_batch = np.array([self.test_obs[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "            act_batch = np.array([self.test_acts[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "            target_batch = np.array([self.test_targets[ip,\n",
    "                                     ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                     for ip, ib in zip(idx_path, idx_batch)])\n",
    "            t_idx_batch = np.array([self.test_task_idx[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "\n",
    "        rs = np.random.RandomState(seed=42)\n",
    "        obs_valid_batch = rs.rand(obs_batch.shape[0], obs_batch.shape[1], 1) < 1 - percentage_imputation\n",
    "        obs_valid_batch[:, :5] = True\n",
    "\n",
    "        data_windows = {'obs': obs_batch, 'act': act_batch, 'target': target_batch, 'obs_valid': obs_valid_batch,\n",
    "                        'normalization': self.normalization,\n",
    "                        'task_index': np.mean(t_idx_batch,-1)}  ###CLEVER TRICK %trajPerTask\n",
    "        # TODO for the target(second half) initialize few things to True\n",
    "\n",
    "        return data_windows\n",
    "\n",
    "    def train_test_split(self, obs, act, delta, grad):\n",
    "        print(obs.shape[0],act.shape[0],delta.shape[0])\n",
    "        assert obs.shape[0] == act.shape[0] == delta.shape[0]\n",
    "        assert isinstance(self._split, list) or isinstance(self._split, float)\n",
    "        episodes = obs.shape[0]\n",
    "\n",
    "        assert len(self._split) == 2\n",
    "        idx_train = self._split[0]\n",
    "        idx_test = self._split[1]\n",
    "        print('Training Indices:', idx_train, 'Testing Indices:', idx_test)\n",
    "\n",
    "        # idx_test = [8,9]\n",
    "\n",
    "        assert len(idx_train) + len(idx_test) <= episodes\n",
    "\n",
    "        return obs[idx_train, :], act[idx_train, :], delta[idx_train, :], grad[idx_train, :], \\\n",
    "               obs[idx_test, :], act[idx_test, :], delta[idx_test, :], grad[idx_test, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7dc20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b155d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b795b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    dataFolder = os.getcwd() + '/data/MobileRobot/sin2/'\n",
    "    # self._trajectoryPath = self._dataFolder + 'HalfCheetahEnv_6c2_cripple.pickle'\n",
    "    trajectoryPath = dataFolder + 'ts_002_50x2000.npz'\n",
    "    data = np.load(trajectoryPath)\n",
    "    print(data.keys())\n",
    "    print(np.sin(data['orn_euler']))\n",
    "    print(np.cos(data['orn_euler']))\n",
    "    print(data['orn_euler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4b13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad1492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabe0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784904fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rohit",
   "language": "python",
   "name": "rohit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
