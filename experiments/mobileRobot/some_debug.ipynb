{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af2a2a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0fda1b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('.')\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from hydra.utils import get_original_cwd\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1230a13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e13b93a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3c7a39f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class metaMobileData(Dataset):\n",
    "    def __init__(self, data_cfg=None):\n",
    "        if data_cfg is None:\n",
    "            raise Exception('Please specify a valid Confg for data')\n",
    "        else:\n",
    "            self.c = data_cfg\n",
    "        if self.c.terrain=='sin2':\n",
    "            self._dataFolder = get_original_cwd() + '/data/MobileRobot/sin2/'\n",
    "            if self.c.frequency == '500':\n",
    "                self._trajectoryPath = self._dataFolder + 'ts_002_50x2000_w_grad.npz'\n",
    "            if self.c.frequency == '240':\n",
    "                self._trajectoryPath = self._dataFolder + 'ts_def_50x1000_w_grad.npz'\n",
    "        else:\n",
    "            self._dataFolder = get_original_cwd() + '/data/MobileRobot/sinMix2/'\n",
    "            if self.c.frequency == '500':\n",
    "                self._trajectoryPath = self._dataFolder + 'ts_002_50x2000_w_grad.npz'\n",
    "            if self.c.frequency == '240':\n",
    "                self._trajectoryPath = self._dataFolder + 'ts_def_50x1000_w_grad.npz'\n",
    "\n",
    "        self._save_windows = self.c.save\n",
    "        self._load_windows = self.c.load\n",
    "        self.dim = self.c.dim\n",
    "        self.trajPerTask = self.c.trajPerTask\n",
    "\n",
    "        self.tar_type = self.c.tar_type\n",
    "\n",
    "        self._split = OmegaConf.to_container(self.c.split)\n",
    "        self._shuffle_split = self.c.shuffle_split\n",
    "\n",
    "        self.meta_batch_size = self.c.meta_batch_size\n",
    "        self.batch_size = self.c.batch_size  # window length/2\n",
    "        self.normalization = None\n",
    "        self.filename = self.c.file_name\n",
    "        self.standardize = self.c.standardize\n",
    "        self.train_windows, self.test_windows = self._load_data()\n",
    "        # data_windows = {'obs': obs_batch, 'act': act_batch, 'target': target_batch, 'obs_valid':obs_valid_batch}\n",
    "\n",
    "    def normalize(self, data, mean, std):\n",
    "        dim = data.shape[-1]\n",
    "        return (data - mean[:self.dim]) / (std[:self.dim] + 1e-10)\n",
    "\n",
    "    def denormalize(self, data, mean, std):\n",
    "        dim = data.shape[-1]\n",
    "        return data * (std[:self.dim] + 1e-10) + mean[:self.dim]\n",
    "\n",
    "    def get_statistics(self, data, dim, difference=False):\n",
    "        if difference:\n",
    "            data = (data[:, 1:, :self.dim] - data[:, :-1, :self.dim])\n",
    "        reshape = lambda x: np.reshape(x, (x.shape[0] * x.shape[1], -1))\n",
    "        data = reshape(data);\n",
    "        mean = np.mean(data, axis=0)\n",
    "        std = np.std(data, axis=0)\n",
    "        return mean, std\n",
    "\n",
    "    def _load_data(self):\n",
    "        # load the pickle file of trajectories\n",
    "        if self._load_windows is not None:\n",
    "            train_data_window = pickle.load(open(self._dataFolder + self.filename + '_train.pickle', 'rb'))\n",
    "            test_data_window = pickle.load(open(self._dataFolder + self.filename + '_test.pickle', 'rb'))\n",
    "            self.normalization = train_data_window['normalization']\n",
    "            print('>>>>>>>>>>>>Loaded Saved Windows with shape<<<<<<<<<<<<<<<', train_data_window['obs'].shape)\n",
    "\n",
    "        else:\n",
    "            data_np = np.load(self._trajectoryPath) #load the .npz data fr\n",
    "            print('>>>>>>>>>>>>Loaded Data Trajectories with shape<<<<<<<<<<<<<<<', data_np['pos'].shape)\n",
    "\n",
    "            # collect obs, act, next states\n",
    "            data = {'observations':[], 'actions':[], 'next_observations':[]}\n",
    "            # dim = np.r_[0:15, 30:41]\n",
    "            data['observations'] = np.concatenate((data_np['pos'][:,:-1,:3],np.sin(data_np['orn_euler'])[:,:-1,:],np.cos(data_np['orn_euler'])[:,:-1,:]),axis=-1)\n",
    "            data['actions'] = data_np['jointAppliedTorques'][:,:-1, :]\n",
    "            data['grad'] = data_np['pos'][:, :-1, 3]\n",
    "            data['next_observations'] = np.concatenate((data_np['pos'][:,1:,:3],np.sin(data_np['orn_euler'])[:,1:,:],np.cos(data_np['orn_euler'])[:,1:,:]),axis=-1)\n",
    "            obs = data['observations']\n",
    "            print('>>>>>>>>>>>>Processed Data Trajectories with shape<<<<<<<<<<<<<<<', obs.shape,data_np['jointAppliedTorques'].shape)\n",
    "            act = data['actions']\n",
    "            next_obs = data['next_observations']\n",
    "            grad = data[\"grad\"]\n",
    "\n",
    "            # train test split\n",
    "            train_obs, train_act, train_next_obs, train_grad, test_obs, test_act, test_next_obs, test_grad = self.train_test_split(obs, act,\n",
    "                                                                                                            next_obs,grad)\n",
    "            train_delta = train_next_obs - train_obs\n",
    "            test_delta = test_next_obs - test_obs\n",
    "\n",
    "            # get different statistics for state, actions, delta_state, delta_action and residuals which will be used for standardization\n",
    "            mean_state_diff, std_state_diff = self.get_statistics(train_delta, self.dim, difference=True)\n",
    "            mean_obs, std_obs = self.get_statistics(train_obs, self.dim)\n",
    "            mean_act, std_act = self.get_statistics(train_act, 2 * self.dim)\n",
    "            self.normalization = dict()\n",
    "            self.normalization['observations'] = [mean_obs, std_obs]\n",
    "            self.normalization['actions'] = [mean_act, std_act]\n",
    "            self.normalization['diff'] = [mean_state_diff, std_state_diff]\n",
    "\n",
    "            # compute delta\n",
    "            if self.tar_type == 'delta':\n",
    "                print(\">>>>>>>>>>>>>>>>>>>>>>>>>>> Training On Differences <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "                self.normalization['targets'] = [mean_state_diff, std_state_diff]\n",
    "            else:\n",
    "                print(\n",
    "                    \">>>>>>>>>>>>>>>>>>>>>>>>>>> Training On Next States(not differences) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "                self.normalization['targets'] = [mean_obs, std_obs]\n",
    "\n",
    "            # Standardize\n",
    "            if self.standardize:\n",
    "                print(\">>>>>>>>>>>>>>>>>>>>>>>>>Standardizing The Data<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "\n",
    "                self.train_obs = self.normalize(train_obs, self.normalization[\"observations\"][0],\n",
    "                                                self.normalization[\"observations\"][1])\n",
    "                self.train_acts = self.normalize(train_act, self.normalization[\"actions\"][0],\n",
    "                                                 self.normalization[\"actions\"][1])\n",
    "\n",
    "                self.test_obs = self.normalize(test_obs, self.normalization[\"observations\"][0],\n",
    "                                               self.normalization[\"observations\"][1])\n",
    "                self.test_acts = self.normalize(test_act, self.normalization[\"actions\"][0],\n",
    "                                                self.normalization[\"actions\"][1])\n",
    "\n",
    "                if self.tar_type == 'delta':\n",
    "                    self.train_targets = self.normalize(train_delta, self.normalization[\"diff\"][0],\n",
    "                                                        self.normalization[\"diff\"][1])\n",
    "                    self.test_targets = self.normalize(test_delta, self.normalization[\"diff\"][0],\n",
    "                                                       self.normalization[\"diff\"][1])\n",
    "                else:\n",
    "                    self.train_targets = self.normalize(train_next_obs, self.normalization[\"observations\"][0],\n",
    "                                                        self.normalization[\"observations\"][1])\n",
    "                    self.test_targets = self.normalize(test_next_obs, self.normalization[\"observations\"][0],\n",
    "                                                       self.normalization[\"observations\"][1])\n",
    "\n",
    "\n",
    "            else:\n",
    "                self.train_obs = train_obs\n",
    "                self.train_acts = train_act\n",
    "                self.test_obs = test_obs\n",
    "                self.test_acts = test_act\n",
    "                if self.tar_type == 'delta':\n",
    "                    self.train_targets = train_delta\n",
    "                    self.test_targets = test_delta\n",
    "                else:\n",
    "                    self.train_targets = train_next_obs\n",
    "                    self.test_targets = test_next_obs\n",
    "            self.train_task_idx = train_grad\n",
    "            self.test_task_idx = test_grad\n",
    "\n",
    "            # Get random windows\n",
    "            train_data_window = self._get_batch(train=True)\n",
    "            test_data_window = self._get_batch(train=False)\n",
    "            if self._save_windows is not None:\n",
    "                pickle.dump(train_data_window, open(self._dataFolder + self.filename + '_train.pickle', 'wb'))\n",
    "                pickle.dump(test_data_window, open(self._dataFolder + self.filename + '_test.pickle', 'wb'))\n",
    "\n",
    "        if isinstance(self._shuffle_split, float):\n",
    "            dataset_size = train_data_window['obs'].shape[0]\n",
    "            indices = np.arange(dataset_size)\n",
    "            print(indices, dataset_size)\n",
    "            np.random.shuffle(indices)\n",
    "            print(indices)\n",
    "            split_idx = int(dataset_size * self._shuffle_split)\n",
    "            idx_train = indices[:split_idx]\n",
    "            idx_test = indices[split_idx:]\n",
    "            train_set = {'obs': [], 'act': [], 'target': [], 'task_index': [], 'normalization': []}\n",
    "            test_set = {'obs': [], 'act': [], 'target': [], 'task_index': [], 'normalization': []}\n",
    "            train_set['obs'] = train_data_window['obs'][idx_train,:,:3];\n",
    "            test_set['obs'] = train_data_window['obs'][idx_test,:,:3];\n",
    "            train_set['act'] = train_data_window['act'][idx_train];\n",
    "            test_set['act'] = train_data_window['act'][idx_test];\n",
    "            train_set['target'] = train_data_window['target'][idx_train,:,:3];\n",
    "            test_set['target'] = train_data_window['target'][idx_test,:,:3];\n",
    "            train_set['task_index'] = train_data_window['task_index'][idx_train,:,3];\n",
    "            test_set['task_index'] = train_data_window['task_index'][idx_test,:,3];\n",
    "            train_set['normalization'] = self.normalization;\n",
    "            test_set['normalization'] = self.normalization\n",
    "            print('Train Test Split Ratio', self._shuffle_split)\n",
    "            return train_set, test_set\n",
    "\n",
    "        return train_data_window, test_data_window\n",
    "\n",
    "\n",
    "    def _get_batch(self, train, percentage_imputation=0.0):\n",
    "        # Takes multiple paths and splits them into windows based on random locations within a trajectory\n",
    "        if train:\n",
    "            num_paths, len_path = self.train_obs.shape[:2]\n",
    "            idx_path = np.random.randint(0, num_paths,\n",
    "                                         size=self.meta_batch_size)  # task index, which gets mixed along the\n",
    "            # process\n",
    "            idx_batch = np.random.randint(self.batch_size, len_path - self.batch_size, size=self.meta_batch_size)\n",
    "\n",
    "            obs_batch = np.array([self.train_obs[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "            act_batch = np.array([self.train_acts[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "            target_batch = np.array([self.train_targets[ip,\n",
    "                                     ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                     for ip, ib in zip(idx_path, idx_batch)])\n",
    "            t_idx_batch = np.array([self.train_task_idx[ip,\n",
    "                                    ib - self.batch_size:ib + self.batch_size]\n",
    "                                    for ip, ib in zip(idx_path, idx_batch)])\n",
    "\n",
    "        else:\n",
    "            num_paths, len_path = self.test_obs.shape[:2]\n",
    "            idx_path = np.random.randint(0, num_paths, size=self.meta_batch_size)\n",
    "            idx_batch = np.random.randint(self.batch_size, len_path - self.batch_size, size=self.meta_batch_size)\n",
    "\n",
    "            obs_batch = np.array([self.test_obs[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "            act_batch = np.array([self.test_acts[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "            target_batch = np.array([self.test_targets[ip,\n",
    "                                     ib - self.batch_size:ib + self.batch_size, :]\n",
    "                                     for ip, ib in zip(idx_path, idx_batch)])\n",
    "            t_idx_batch = np.array([self.test_task_idx[ip,\n",
    "                                  ib - self.batch_size:ib + self.batch_size]\n",
    "                                  for ip, ib in zip(idx_path, idx_batch)])\n",
    "\n",
    "        rs = np.random.RandomState(seed=42)\n",
    "        obs_valid_batch = rs.rand(obs_batch.shape[0], obs_batch.shape[1], 1) < 1 - percentage_imputation\n",
    "        obs_valid_batch[:, :5] = True\n",
    "\n",
    "        data_windows = {'obs': obs_batch, 'act': act_batch, 'target': target_batch, 'obs_valid': obs_valid_batch,\n",
    "                        'normalization': self.normalization,\n",
    "                        'task_index': np.mean(t_idx_batch,-1)}  ###CLEVER TRICK %trajPerTask\n",
    "        # TODO for the target(second half) initialize few things to True\n",
    "\n",
    "        return data_windows\n",
    "\n",
    "    def train_test_split(self, obs, act, delta, grad):\n",
    "        print(obs.shape[0],act.shape[0],delta.shape[0])\n",
    "        assert obs.shape[0] == act.shape[0] == delta.shape[0]\n",
    "        assert isinstance(self._split, list) or isinstance(self._split, float)\n",
    "        episodes = obs.shape[0]\n",
    "\n",
    "        assert len(self._split) == 2\n",
    "        idx_train = self._split[0]\n",
    "        idx_test = self._split[1]\n",
    "        print('Training Indices:', idx_train, 'Testing Indices:', idx_test)\n",
    "\n",
    "        # idx_test = [8,9]\n",
    "\n",
    "        assert len(idx_train) + len(idx_test) <= episodes\n",
    "\n",
    "        return obs[idx_train, :], act[idx_train, :], delta[idx_train, :], grad[idx_train, :], \\\n",
    "               obs[idx_test, :], act[idx_test, :], delta[idx_test, :], grad[idx_test, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c7dc20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79b155d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23b795b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x7ff4390d47d0>)\n",
      "[[[-1.87221911e-05 -4.26430594e-02  3.71033138e-06]\n",
      "  [-1.90942578e-05 -4.34857699e-02  3.79171458e-06]\n",
      "  [-1.94666134e-05 -4.43288896e-02  3.87345301e-06]\n",
      "  ...\n",
      "  [-1.08461183e-04  1.65951306e-01 -3.25643621e-03]\n",
      "  [-1.65225740e-04  1.67743412e-01 -3.25876662e-03]\n",
      "  [-1.76083972e-04  1.69558887e-01 -3.25771456e-03]]\n",
      "\n",
      " [[-1.65184812e-05 -3.76408302e-02  3.23404593e-06]\n",
      "  [-1.68449043e-05 -3.83809907e-02  3.30388923e-06]\n",
      "  [-1.71715502e-05 -3.91214679e-02  3.37400593e-06]\n",
      "  ...\n",
      "  [-1.27586040e-03 -2.22571665e-01  4.04456655e-03]\n",
      "  [-1.38144994e-03 -2.21449456e-01  4.06020347e-03]\n",
      "  [-1.46614140e-03 -2.20313648e-01  4.06762874e-03]]\n",
      "\n",
      " [[-2.17341178e-05 -4.94703612e-02  4.37806023e-06]\n",
      "  [-2.21689129e-05 -5.04535053e-02  4.47597836e-06]\n",
      "  [-2.26041024e-05 -5.14372015e-02  4.57438169e-06]\n",
      "  ...\n",
      "  [ 6.56318952e-07  2.94274671e-01 -8.07648936e-04]\n",
      "  [ 5.91791898e-05  2.93051721e-01 -7.96141366e-04]\n",
      "  [ 9.00750063e-05  2.91759848e-01 -7.74971631e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.66847224e-05 -3.80183768e-02  3.26961241e-06]\n",
      "  [-1.70145813e-05 -3.87662670e-02  3.34030980e-06]\n",
      "  [-1.73446678e-05 -3.95144805e-02  3.41128638e-06]\n",
      "  ...\n",
      "  [-1.29282078e-03 -2.07774378e-01  4.14436772e-03]\n",
      "  [-1.09242528e-03 -2.06412293e-01  4.12880160e-03]\n",
      "  [-9.21132394e-04 -2.05061910e-01  4.11868765e-03]]\n",
      "\n",
      " [[-1.76727499e-05 -4.02616201e-02  3.48221949e-06]\n",
      "  [-1.80230541e-05 -4.10554722e-02  3.55804840e-06]\n",
      "  [-1.83736146e-05 -4.18496881e-02  3.63419219e-06]\n",
      "  ...\n",
      "  [ 1.32903616e-03 -4.30315881e-02 -5.58325699e-03]\n",
      "  [ 1.19790741e-03 -4.12888589e-02 -5.57155688e-03]\n",
      "  [ 1.06912427e-03 -3.95545852e-02 -5.56354770e-03]]\n",
      "\n",
      " [[-2.01758132e-05 -4.59394911e-02  4.03018235e-06]\n",
      "  [-2.05781038e-05 -4.68499410e-02  4.11943567e-06]\n",
      "  [-2.09807320e-05 -4.77608667e-02  4.20910432e-06]\n",
      "  ...\n",
      "  [ 1.45976282e-04  3.29083670e-01  1.12960409e-03]\n",
      "  [ 2.37900728e-04  3.29624269e-01  1.16436121e-03]\n",
      "  [ 3.16795997e-04  3.30204185e-01  1.19711392e-03]]]\n",
      "[[[1.         0.99909037 1.        ]\n",
      "  [1.         0.99905405 1.        ]\n",
      "  [1.         0.99901699 1.        ]\n",
      "  ...\n",
      "  [0.99999999 0.98613395 0.9999947 ]\n",
      "  [0.99999999 0.98583069 0.99999469]\n",
      "  [0.99999998 0.98552006 0.99999469]]\n",
      "\n",
      " [[1.         0.99929133 1.        ]\n",
      "  [1.         0.99926318 1.        ]\n",
      "  [1.         0.99923446 1.        ]\n",
      "  ...\n",
      "  [0.99999919 0.97491633 0.99999182]\n",
      "  [0.99999905 0.97517185 0.99999176]\n",
      "  [0.99999893 0.97542908 0.99999173]]\n",
      "\n",
      " [[1.         0.99877559 1.        ]\n",
      "  [1.         0.99872641 1.        ]\n",
      "  [1.         0.99867623 1.        ]\n",
      "  ...\n",
      "  [1.         0.95572089 0.99999967]\n",
      "  [1.         0.95609659 0.99999968]\n",
      "  [1.         0.95649161 0.9999997 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.         0.99927704 1.        ]\n",
      "  [1.         0.99924831 1.        ]\n",
      "  [1.         0.999219   1.        ]\n",
      "  ...\n",
      "  [0.99999916 0.97817678 0.99999141]\n",
      "  [0.9999994  0.97846511 0.99999148]\n",
      "  [0.99999958 0.978749   0.99999152]]\n",
      "\n",
      " [[1.         0.99918917 1.        ]\n",
      "  [1.         0.99915687 1.        ]\n",
      "  [1.         0.99912392 1.        ]\n",
      "  ...\n",
      "  [0.99999912 0.99907371 0.99998441]\n",
      "  [0.99999928 0.99914725 0.99998448]\n",
      "  [0.99999943 0.99921741 0.99998452]]\n",
      "\n",
      " [[1.         0.99894422 1.        ]\n",
      "  [1.         0.99890194 1.        ]\n",
      "  [1.         0.9988588  1.        ]\n",
      "  ...\n",
      "  [0.99999999 0.94430077 0.99999936]\n",
      "  [0.99999997 0.9441122  0.99999932]\n",
      "  [0.99999995 0.94390953 0.99999928]]]\n",
      "[[[-1.87221911e-05 -4.26559939e-02  3.71033138e-06]\n",
      "  [-1.90942578e-05 -4.34994869e-02  3.79171458e-06]\n",
      "  [-1.94666134e-05 -4.43434205e-02  3.87345301e-06]\n",
      "  ...\n",
      "  [-1.08461183e-04  1.66722616e-01 -3.25644196e-03]\n",
      "  [-1.65225741e-04  1.68540199e-01 -3.25877239e-03]\n",
      "  [-1.76083973e-04  1.70382057e-01 -3.25772032e-03]]\n",
      "\n",
      " [[-1.65184812e-05 -3.76497243e-02  3.23404593e-06]\n",
      "  [-1.68449043e-05 -3.83904201e-02  3.30388923e-06]\n",
      "  [-1.71715502e-05 -3.91314539e-02  3.37400593e-06]\n",
      "  ...\n",
      "  [-1.27586074e-03 -2.24451511e-01  4.04457757e-03]\n",
      "  [-1.38145038e-03 -2.23300580e-01  4.06021462e-03]\n",
      "  [-1.46614193e-03 -2.22136008e-01  4.06763995e-03]]\n",
      "\n",
      " [[-2.17341178e-05 -4.94905618e-02  4.37806023e-06]\n",
      "  [-2.21689129e-05 -5.04749352e-02  4.47597836e-06]\n",
      "  [-2.26041024e-05 -5.14599105e-02  4.57438169e-06]\n",
      "  ...\n",
      "  [ 6.56318952e-07  2.98696496e-01 -8.07649024e-04]\n",
      "  [ 5.91791898e-05  2.97417137e-01 -7.96141450e-04]\n",
      "  [ 9.00750064e-05  2.96066222e-01 -7.74971709e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.66847224e-05 -3.80275414e-02  3.26961241e-06]\n",
      "  [-1.70145813e-05 -3.87759834e-02  3.34030980e-06]\n",
      "  [-1.73446678e-05 -3.95247706e-02  3.41128638e-06]\n",
      "  ...\n",
      "  [-1.29282114e-03 -2.09299132e-01  4.14437958e-03]\n",
      "  [-1.09242550e-03 -2.07906864e-01  4.12881333e-03]\n",
      "  [-9.21132525e-04 -2.06526961e-01  4.11869930e-03]]\n",
      "\n",
      " [[-1.76727499e-05 -4.02725054e-02  3.48221949e-06]\n",
      "  [-1.80230541e-05 -4.10670145e-02  3.55804840e-06]\n",
      "  [-1.83736146e-05 -4.18619137e-02  3.63419219e-06]\n",
      "  ...\n",
      "  [ 1.32903655e-03 -4.30448796e-02 -5.58328600e-03]\n",
      "  [ 1.19790769e-03 -4.13005992e-02 -5.57158571e-03]\n",
      "  [ 1.06912448e-03 -3.95649068e-02 -5.56357640e-03]]\n",
      "\n",
      " [[-2.01758132e-05 -4.59556652e-02  4.03018235e-06]\n",
      "  [-2.05781038e-05 -4.68670965e-02  4.11943567e-06]\n",
      "  [-2.09807320e-05 -4.77790433e-02  4.20910432e-06]\n",
      "  ...\n",
      "  [ 1.45976283e-04  3.35333031e-01  1.12960433e-03]\n",
      "  [ 2.37900730e-04  3.35905574e-01  1.16436148e-03]\n",
      "  [ 3.16796002e-04  3.36519885e-01  1.19711421e-03]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    dataFolder = os.getcwd() + '/data/MobileRobot/sin2/'\n",
    "    # self._trajectoryPath = self._dataFolder + 'HalfCheetahEnv_6c2_cripple.pickle'\n",
    "    trajectoryPath = dataFolder + 'ts_002_50x2000.npz'\n",
    "    data = np.load(trajectoryPath)\n",
    "    print(data.keys())\n",
    "    print(np.sin(data['orn_euler']))\n",
    "    print(np.cos(data['orn_euler']))\n",
    "    print(data['orn_euler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4b13c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "784904fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Applications/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug', '/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev', '/Users/I562243/Documents/to_send_GD/code/kit/HiP-RSSM-Internal', '/Users/I562243/Documents/to_send_GD/code/kit/HiP-RSSM-Internal', '/Users/I562243/opt/anaconda3/envs/Rohit/lib/python37.zip', '/Users/I562243/opt/anaconda3/envs/Rohit/lib/python3.7', '/Users/I562243/opt/anaconda3/envs/Rohit/lib/python3.7/lib-dynload', '', '/Users/I562243/opt/anaconda3/envs/Rohit/lib/python3.7/site-packages', '/Users/I562243/opt/anaconda3/envs/Rohit/lib/python3.7/site-packages/IPython/extensions', '/Users/I562243/.ipython', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '/Users/I562243/Documents/to_send_GD/code/kit/HiP-RSSM-Internal', '.', '/Users/I562243/Documents/to_send_GD/code/kit/HiP-RSSM-Internal', '.', '/Users/I562243/Documents/to_send_GD/code/kit/HiP-RSSM-Internal', './experiments/mobileRobot/']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append('./experiments/mobileRobot/')\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import pickle\n",
    "\n",
    "from data.mobileDataSeq import metaMobileData\n",
    "from data.mobileDataSeq_Infer import metaMobileDataInfer\n",
    "from meta_dynamic_models.neural_process_dynamics.neural_process.setFunctionContext import SetEncoder\n",
    "from meta_dynamic_models.neural_process_dynamics.neural_process_ssm.recurrentEncoderDecoder import acrknContextualDecoder\n",
    "from meta_dynamic_models.neural_process_dynamics.npDynamics import npDyn\n",
    "from learning import hiprssm_dyn_trainer\n",
    "from inference import hiprssm_dyn_inference\n",
    "from utils.metrics import naive_baseline\n",
    "from utils.dataProcess import split_k_m\n",
    "from utils.metrics import root_mean_squared\n",
    "from utils.latentVis import plot_clustering, plot_clustering_1d\n",
    "\n",
    "nn = torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides [overrides ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "def generate_mobile_robot_data_set(data, dim):\n",
    "    train_windows, test_windows = data.train_windows, data.test_windows\n",
    "\n",
    "    train_targets = train_windows['target'][:,:,:dim]\n",
    "    test_targets = test_windows['target'][:,:,:dim]\n",
    "\n",
    "    train_obs = train_windows['obs'][:,:,:dim]\n",
    "    test_obs = test_windows['obs'][:,:,:dim]\n",
    "\n",
    "    train_task_idx = train_windows['task_index']\n",
    "    test_task_idx = test_windows['task_index']\n",
    "\n",
    "    train_act = train_windows['act'][:,:,:dim]\n",
    "    test_act = test_windows['act'][:,:,:dim]\n",
    "    print(test_act.shape, train_act.shape)\n",
    "\n",
    "    return torch.from_numpy(train_obs).float(), torch.from_numpy(train_act).float(), torch.from_numpy(train_targets).float(), torch.from_numpy(train_task_idx).float(),\\\n",
    "           torch.from_numpy(test_obs).float(), torch.from_numpy(test_act).float(), torch.from_numpy(test_targets).float(), torch.from_numpy(test_task_idx).float()\n",
    "\n",
    "@hydra.main(config_path='conf',config_name='config')\n",
    "def my_app(cfg)->OmegaConf:\n",
    "    global config\n",
    "    model_cfg = cfg.model\n",
    "    exp = Experiment(model_cfg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    my_app()\n",
    "\n",
    "\n",
    "\n",
    "## https://stackoverflow.com/questions/32761999/how-to-pass-an-entire-list-as-command-line-argument-in-python/32763023\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "\n",
    "class Experiment():\n",
    "    def __init__(self, cfg):\n",
    "        self.global_cfg = cfg\n",
    "        self._experiment()\n",
    "\n",
    "\n",
    "    def _experiment(self):\n",
    "        \"\"\"Data\"\"\"\n",
    "        cfg = self.global_cfg\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        tar_type = cfg.data_reader.tar_type  # 'delta' - if to train on differences to current states\n",
    "        # 'next_state' - if to trian directly on the  next states\n",
    "\n",
    "        data = metaMobileData(cfg.data_reader)\n",
    "\n",
    "        train_obs, train_act, train_targets, train_task_idx, test_obs, test_act, test_targets, test_task_idx = generate_mobile_robot_data_set(\n",
    "            data,  cfg.data_reader.dim)\n",
    "        act_dim = train_act.shape[-1]\n",
    "\n",
    "        \"\"\"Naive Baseline\"\"\"\n",
    "        naive_baseline(test_obs[:, :-1, :], test_obs[:, 1:, :], steps=[1, 3, 5, 7, 10], data=data, denorma=True)\n",
    "\n",
    "        ####\n",
    "        impu = cfg.data_reader.imp\n",
    "\n",
    "        save_path = os.getcwd() + '/experiments/saved_models/' + cfg.wandb.exp_name + '.ckpt'\n",
    "\n",
    "        ##### Define WandB Stuffs\n",
    "        expName = cfg.wandb.exp_name\n",
    "        if cfg.wandb.log:\n",
    "            mode = \"online\"\n",
    "        else:\n",
    "            mode = \"disabled\"\n",
    "\n",
    "        ## Initializing wandb object and sweep object\n",
    "        wandb_run = wandb.init(project=cfg.wandb.project_name, name=expName,\n",
    "                               mode=mode)  # wandb object has a set of configs associated with it as well\n",
    "\n",
    "        ### Model, Train and Inference Modules\n",
    "        encoder = SetEncoder(\n",
    "            train_obs.shape[-1] + train_act.shape[-1] + train_targets.shape[-1],\n",
    "            lod=cfg.np.agg_dim, config=cfg.set_encoder)\n",
    "\n",
    "\n",
    "        decoder = acrknContextualDecoder(ltd=cfg.np.agg_dim*2, target_dim=train_targets.shape[-1],\n",
    "                                         lod=cfg.np.latent_obs_dim,\n",
    "                                         lad=train_act.shape[-1], config=cfg.ssm_decoder)\n",
    "\n",
    "        np_model = npDyn(encoder, decoder, dec_type='acrkn', config=cfg.np)\n",
    "        np_learn = hiprssm_dyn_trainer.Learn(np_model, loss=cfg.learn.loss, imp=impu, config=cfg, run=wandb_run,\n",
    "                                           log=cfg.wandb['log'])\n",
    "\n",
    "        if cfg.learn.load == False:\n",
    "            #### Train the Model\n",
    "            np_learn.train(train_obs, train_act, train_targets, train_task_idx, cfg.learn.epochs, cfg.learn.batch_size,\n",
    "                           test_obs, test_act,\n",
    "                           test_targets, test_task_idx)\n",
    "\n",
    "        if not cfg.wandb.sweep:\n",
    "            ##### Load best model\n",
    "            model_at = wandb_run.use_artifact('saved_model' + ':latest')\n",
    "            model_path = model_at.download()  ###return the save durectory path in wandb local\n",
    "            np_model.load_state_dict(torch.load(save_path))\n",
    "            print('>>>>>>>>>>Loaded The Model From Local Folder<<<<<<<<<<<<<<<<<<<')\n",
    "\n",
    "            ###### Inference\n",
    "\n",
    "            ##########  Initialize inference class\n",
    "            np_infer = hiprssm_dyn_inference.Infer(np_model, data=data, config=cfg, run=wandb_run)\n",
    "            batch_size = 10\n",
    "            k = int(train_obs.shape[1] / 2)\n",
    "            pred_mean, pred_var, gt, obs_valid, _, _, cur_obs = np_infer.predict(test_obs, test_act, test_targets, test_task_idx,\n",
    "                                                                        imp=impu, k=k,\n",
    "                                                                        test_gt_known=True, batch_size=batch_size, tar=tar_type)\n",
    "            print(pred_mean.shape, pred_var.shape, gt.shape, obs_valid.shape)\n",
    "\n",
    "\n",
    "\n",
    "            rmse_next_state, pred_obs, gt_obs = root_mean_squared(pred_mean, gt, data,\n",
    "                                                                      tar=\"observations\", denorma=True)\n",
    "            wandb_run.summary['rmse_denorma_next_state'] = rmse_next_state\n",
    "\n",
    "            print(\"Root mean square Error is:\", rmse_next_state)\n",
    "\n",
    "\n",
    "            multiSteps = [1,3,5,10,20,30,40,50]\n",
    "            for step in multiSteps:\n",
    "                 pred_mean, pred_var, gt_multi = np_infer.predict_mbrl(test_obs, test_act, test_targets, k=k,\n",
    "                                                                 batch_size=batch_size,\n",
    "                                                                 multiStep=step, tar=tar_type)\n",
    "\n",
    "                 rmse_next_state, pred_obs, gt_obs = root_mean_squared(pred_mean, gt_multi, data, tar=\"observations\", denorma=True)\n",
    "                 print(step,rmse_next_state)\n",
    "                 wandb_run.summary['rmse_multi_step_' + str(step)] = rmse_next_state\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rohit",
   "language": "python",
   "name": "rohit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}