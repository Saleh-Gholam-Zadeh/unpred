wandb:
  log: True
  #project_name: 'Probabilistic_pred_per_cpu_'
  #project_name: 'Model_comparison_per_cpu_'
  #project_name: 'visualize_residual'
  #project_name: 'compute_residual'
  project_name: 'DE1_multistep'
  #exp_name: 'MLPL1_DE1_30'
  #exp_name: 'tmp_stack'
  #exp_name: 'MLPL1_DE1_halfsize'
  #exp_name: 'MLPL2_on_MLPL1_DE1_halfsize'
  exp_name: 'NSTL1_DE1_30'
  #exp_name: 'MLPL1_DE1'
  #exp_name: 'MLPL2_on_MLPL1_DE1_30'
  #exp_name: 'NSTL1_on_MLPL2_on_MLPL1_DE1_30'
  sweep: False
  sweep_id: null

learn:
  log_stat: False
  num_class: 'null'
  seed: 1
  #load_dir_residual : 'MLPL2_on_MLPL1_DE1_30_ctx-60to30'
  load_dir_residual : 'MLPL1_DE1_30'
  train_residual: False  #  for NST TO not add the mean of context to prediction when train on residuals #False --> default behavior of NST
  #model_name: 'MLPL1_DE1_halfsize' # otherway around run it on LAPTOP
  #model_name: 'MLPL2_on_MLPL1_DE1_halfsize' # otherway around run it on LAPTOP
  #model_name: 'MLPL2_DE1_halfsize'
  #model_name: 'MLPL2_on_MLPL1_DE1_30'
  #model_name: 'NSTL1_on_MLPL2_on_MLPL1_DE1_30'
  model_name: 'NSTL1_DE1_30'

  load: False # For stacking we train the 2nd model from scratch so we don't need to load a model
  #gpu: '0'
  #epochs: 250
  epochs: 200

  batch_size: 450
  #batch_size: 150
  latent_vis: False
  plot_traj: False
  #latent_vis: True
  #plot_traj: True
  #lr: 8e-5
  lr: 1e-4
  save_model: True
  #loss: 'nll'  /// 'kl_rmse'
  loss: 'mae'
  #loss: 'mse'
  #loss: 'cross_entropy'
  #loss: 'kl_rmse'  # KL + packet_rmse
  #loss: 'kl_cross' kl(dist) + cross_entropy entropy on packets
  features_in: 'all'  # or 'packets'
  #features_out: 'all' # or 'packets'
  #features_in: 'packets'
  features_out: 'packets'

transformer_arch:
  enc_layer: 2
  dec_layer: 1
  n_head: 8
  d_model: 128
  dropout: 0.1

#  enc_layer: 2
#  dec_layer: 1
#  n_head: 4
#  d_model: 128
#  dropout: 0.1

#  enc_layer: 1
#  dec_layer: 1
#  n_head: 2
#  d_model: 40
#  dropout: 0.1

  factor: 3
  #seq_len: 60 --> not needed--> the value of context_size will be taken  # contex_size
  p_hidden_layers: 2
  p_hidden_dims: [128 ,128]  # AZ Tu ns_transformer.py taghir mikone faghat
  #p_hidden_dims: [24 ,24]


np:
  clip_gradients: True
  latent_obs_dim: 30  #orig
  #latent_obs_dim: 60
  agg_dim: 60  #orig
  #agg_dim: 120


data_reader:
  portion: 0
  imp: 0 # 75% for longer step ahead. for single can be set to 0
  terrain: 'sin2'
  frequency: '500'
  meta_batch_size: 300000   #400000 --> 162Gb RAM
  #meta_batch_size: 150000

  #batch_size: 150
  #batch_size: 64 #context_size
  context_size: 60 #context_size
  pred_len: 30
  #pred_len: 1
  #tar_type: 'delta' # Use "delta" argument to predict on the differences as targets. In other cases use "observations".
  tar_type: 'observations'
  #load: null
  load: True
  save: 1
  dim: 111
  standardize: True  #havaset bashe target distribution ro normalize nakoni
  standardize_dist: False
  #flows: ['flow_48'] # a list for per_flow model
  flows: None #---> single model for all flows
  #standardize: False


  split:
    - [ 0,2,4,6,8,10,12,14,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,41,42,43,44,45,46,47,48,49 ]
    - [ 1,3,5,7,9,11,13,15,39,40 ]


  shuffle_split: null
  file_name: 'MobileWindows'
  trajPerTask: 10

submitit:
  folder: './experiments/mujoco_meta/Ant/slurm_output/'
  name: 'jobtrial'
  timeout_min: 10
  mem_gb: 10
  nodes: 1
  cpus_per_task: 3
  gpus_per_node: 1
  tasks_per_node: 1
  slurm_partition: gpu
